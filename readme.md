# DiT Transformer Implementation

This is an implementation of the Diffusion Transformer Architecture based on [this paper](https://arxiv.org/pdf/2212.09748) which was based on stable diffusion and ViT.

Some questions that I have and will attempt to answer is:

- How do attention layers 'beat' the learning/representational capacity of CNNs, which seem well suited for images (inductive bias of way animals see)?
